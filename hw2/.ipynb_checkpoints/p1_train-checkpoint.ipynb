{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torchvision.models as models\n",
    "import copy\n",
    "from torchvision.utils import save_image\n",
    "import PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_transform = transforms.Compose([torchvision.transforms.RandomAffine(degrees = 4.5, scale = (0.9, 1.1), shear = 4),\n",
    "                                    torchvision.transforms.RandomHorizontalFlip(0.5),\n",
    "                                    transforms.ToTensor()])\n",
    "\n",
    "vgg19_bn = models.vgg19_bn(pretrained=True)\n",
    "\n",
    "def getLabelAndIndexFromFileName(fn):\n",
    "    _fn = fn.split('/').pop()\n",
    "    _list = _fn.split(\"_\")\n",
    "    label = int(_list[0])\n",
    "    index = int(_list[1].split(\".\")[0])\n",
    "    return label, index\n",
    "\n",
    "class imgData(Dataset):\n",
    "    def __init__(self, root, transform = None):\n",
    "        self.filenames = []\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        filenames = glob.glob(os.path.join(self.root, '*.png'))\n",
    "        for fn in filenames:\n",
    "            label, index = getLabelAndIndexFromFileName(fn)\n",
    "            self.filenames.append((fn, label)) # (filename, label) pair\n",
    "        self.len = len(self.filenames) \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        with torch.no_grad():\n",
    "            fn, label = self.filenames[index]\n",
    "            image = Image.open(fn)\n",
    "            if self.transform is not None:\n",
    "                image = self.transform(image)\n",
    "            return image, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# images in trainset: 22500\n",
      "# images in testset: 2500\n",
      "Image tensor in each batch: torch.Size([2048, 3, 32, 32]) torch.float32\n",
      "Label tensor in each batch: torch.Size([2048]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "trainset = imgData(root='hw2_data/p1_data/train_50', transform=aug_transform)\n",
    "testset = imgData(root='hw2_data/p1_data/val_50', transform=transforms.ToTensor())\n",
    "\n",
    "print('# images in trainset:', len(trainset))\n",
    "print('# images in testset:', len(testset))\n",
    "\n",
    "# Use the torch dataloader to iterate through the dataset\n",
    "trainset_loader = DataLoader(trainset, batch_size=2048, shuffle=True, num_workers=0)\n",
    "testset_loader = DataLoader(testset, batch_size=2500, shuffle=False, num_workers=0)\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(trainset_loader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "print('Image tensor in each batch:', images.shape, images.dtype)\n",
    "print('Label tensor in each batch:', labels.shape, labels.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device used: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# activate cuda\n",
    "use_cuda = torch.cuda.is_available()\n",
    "torch.manual_seed(123)\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "#device = torch.device('cpu')\n",
    "print('Device used:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            vgg19_bn\n",
    "        )\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(1000, 50)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.fc1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epoch, log_interval=2):\n",
    "    optimizer = optim.SGD(model.parameters(), lr = 0.001, momentum = 0.9, weight_decay = 1e-3)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    model.train()\n",
    "    iteration = 0\n",
    "    for ep in range(epoch):\n",
    "        print('')\n",
    "        for batch_idx, (data, target) in enumerate(trainset_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if iteration % log_interval == 0:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    ep, batch_idx * len(data), len(trainset_loader.dataset),\n",
    "                    100. * batch_idx / len(trainset_loader), loss.item()))\n",
    "            iteration += 1\n",
    "        test(model) # Evaluate at the end of each epoch\n",
    "\n",
    "def save_checkpoint(checkpoint_path, model, optimizer):\n",
    "    state = {'state_dict': model.state_dict(),\n",
    "             'optimizer' : optimizer.state_dict()}\n",
    "    torch.save(state, checkpoint_path)\n",
    "    print('model saved to %s' % checkpoint_path)       \n",
    "        \n",
    "currentBestPercentage = 0  \n",
    "def test(model):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    model.eval()  # Important: set evaluation mode\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad(): # This will free the GPU memory used for back-prop\n",
    "        for data, target in testset_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item()*len(data) # sum up batch loss\n",
    "            pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    test_loss /= len(testset_loader.dataset)\n",
    "    percentage = 100. * correct / len(testset_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(test_loss, correct, len(testset_loader.dataset), percentage))\n",
    "          \n",
    "    global currentBestPercentage\n",
    "    if currentBestPercentage < percentage and 60.0 < percentage:\n",
    "        print('current best model updated!')\n",
    "        currentBestPercentage = percentage\n",
    "        optimizer = optim.SGD(model.parameters(), lr = 0.001, momentum = 0.9, weight_decay = 1e-3)\n",
    "        save_checkpoint('p1_current_best-'+str(round(percentage,4))+'.pth', model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (features): Sequential(\n",
      "    (0): VGG(\n",
      "      (features): Sequential(\n",
      "        (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU(inplace=True)\n",
      "        (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "        (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (9): ReLU(inplace=True)\n",
      "        (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (12): ReLU(inplace=True)\n",
      "        (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "        (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (16): ReLU(inplace=True)\n",
      "        (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (19): ReLU(inplace=True)\n",
      "        (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (22): ReLU(inplace=True)\n",
      "        (23): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (24): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (25): ReLU(inplace=True)\n",
      "        (26): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "        (27): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (29): ReLU(inplace=True)\n",
      "        (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (32): ReLU(inplace=True)\n",
      "        (33): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (34): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (35): ReLU(inplace=True)\n",
      "        (36): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (37): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (38): ReLU(inplace=True)\n",
      "        (39): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "        (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (42): ReLU(inplace=True)\n",
      "        (43): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (44): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (45): ReLU(inplace=True)\n",
      "        (46): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (47): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (48): ReLU(inplace=True)\n",
      "        (49): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (50): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (51): ReLU(inplace=True)\n",
      "        (52): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "      (classifier): Sequential(\n",
      "        (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "        (1): ReLU(inplace=True)\n",
      "        (2): Dropout(p=0.5, inplace=False)\n",
      "        (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "        (4): ReLU(inplace=True)\n",
      "        (5): Dropout(p=0.5, inplace=False)\n",
      "        (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc1): Sequential(\n",
      "    (0): Linear(in_features=1000, out_features=50, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = Net()\n",
    "model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Epoch: 0 [0/22500 (0%)]\tLoss: 5.028567\n",
      "Train Epoch: 0 [4096/22500 (18%)]\tLoss: 4.927456\n",
      "Train Epoch: 0 [8192/22500 (36%)]\tLoss: 4.574163\n",
      "Train Epoch: 0 [12288/22500 (55%)]\tLoss: 4.283083\n",
      "Train Epoch: 0 [16384/22500 (73%)]\tLoss: 4.061073\n",
      "Train Epoch: 0 [20200/22500 (91%)]\tLoss: 3.972920\n",
      "\n",
      "Test set: Average loss: 4.2301, Accuracy: 88/2500 (4%)\n",
      "\n",
      "\n",
      "Train Epoch: 1 [2048/22500 (9%)]\tLoss: 4.080387\n",
      "Train Epoch: 1 [6144/22500 (27%)]\tLoss: 3.940900\n",
      "Train Epoch: 1 [10240/22500 (45%)]\tLoss: 3.914683\n",
      "Train Epoch: 1 [14336/22500 (64%)]\tLoss: 3.924901\n",
      "Train Epoch: 1 [18432/22500 (82%)]\tLoss: 3.909577\n",
      "\n",
      "Test set: Average loss: 3.8917, Accuracy: 75/2500 (3%)\n",
      "\n",
      "\n",
      "Train Epoch: 2 [0/22500 (0%)]\tLoss: 3.908636\n",
      "Train Epoch: 2 [4096/22500 (18%)]\tLoss: 3.901398\n",
      "Train Epoch: 2 [8192/22500 (36%)]\tLoss: 3.898546\n",
      "Train Epoch: 2 [12288/22500 (55%)]\tLoss: 3.892757\n",
      "Train Epoch: 2 [16384/22500 (73%)]\tLoss: 3.865944\n",
      "Train Epoch: 2 [20200/22500 (91%)]\tLoss: 3.819874\n",
      "\n",
      "Test set: Average loss: 3.7433, Accuracy: 146/2500 (6%)\n",
      "\n",
      "\n",
      "Train Epoch: 3 [2048/22500 (9%)]\tLoss: 3.803493\n",
      "Train Epoch: 3 [6144/22500 (27%)]\tLoss: 3.749256\n",
      "Train Epoch: 3 [10240/22500 (45%)]\tLoss: 3.675670\n",
      "Train Epoch: 3 [14336/22500 (64%)]\tLoss: 3.595202\n",
      "Train Epoch: 3 [18432/22500 (82%)]\tLoss: 3.495961\n",
      "\n",
      "Test set: Average loss: 3.3047, Accuracy: 366/2500 (15%)\n",
      "\n",
      "\n",
      "Train Epoch: 4 [0/22500 (0%)]\tLoss: 3.425756\n",
      "Train Epoch: 4 [4096/22500 (18%)]\tLoss: 3.360585\n",
      "Train Epoch: 4 [8192/22500 (36%)]\tLoss: 3.257286\n",
      "Train Epoch: 4 [12288/22500 (55%)]\tLoss: 3.207735\n",
      "Train Epoch: 4 [16384/22500 (73%)]\tLoss: 3.064382\n",
      "Train Epoch: 4 [20200/22500 (91%)]\tLoss: 3.009860\n",
      "\n",
      "Test set: Average loss: 2.8133, Accuracy: 601/2500 (24%)\n",
      "\n",
      "\n",
      "Train Epoch: 5 [2048/22500 (9%)]\tLoss: 2.939967\n",
      "Train Epoch: 5 [6144/22500 (27%)]\tLoss: 2.814736\n",
      "Train Epoch: 5 [10240/22500 (45%)]\tLoss: 2.755028\n",
      "Train Epoch: 5 [14336/22500 (64%)]\tLoss: 2.729766\n",
      "Train Epoch: 5 [18432/22500 (82%)]\tLoss: 2.610675\n",
      "\n",
      "Test set: Average loss: 2.3840, Accuracy: 793/2500 (32%)\n",
      "\n",
      "\n",
      "Train Epoch: 6 [0/22500 (0%)]\tLoss: 2.568948\n",
      "Train Epoch: 6 [4096/22500 (18%)]\tLoss: 2.481102\n",
      "Train Epoch: 6 [8192/22500 (36%)]\tLoss: 2.450540\n",
      "Train Epoch: 6 [12288/22500 (55%)]\tLoss: 2.430977\n",
      "Train Epoch: 6 [16384/22500 (73%)]\tLoss: 2.375494\n",
      "Train Epoch: 6 [20200/22500 (91%)]\tLoss: 2.285264\n",
      "\n",
      "Test set: Average loss: 2.1401, Accuracy: 906/2500 (36%)\n",
      "\n",
      "\n",
      "Train Epoch: 7 [2048/22500 (9%)]\tLoss: 2.276232\n",
      "Train Epoch: 7 [6144/22500 (27%)]\tLoss: 2.214458\n",
      "Train Epoch: 7 [10240/22500 (45%)]\tLoss: 2.151348\n",
      "Train Epoch: 7 [14336/22500 (64%)]\tLoss: 2.199081\n",
      "Train Epoch: 7 [18432/22500 (82%)]\tLoss: 2.158243\n",
      "\n",
      "Test set: Average loss: 1.9250, Accuracy: 1040/2500 (42%)\n",
      "\n",
      "\n",
      "Train Epoch: 8 [0/22500 (0%)]\tLoss: 2.018621\n",
      "Train Epoch: 8 [4096/22500 (18%)]\tLoss: 2.007263\n",
      "Train Epoch: 8 [8192/22500 (36%)]\tLoss: 2.040015\n",
      "Train Epoch: 8 [12288/22500 (55%)]\tLoss: 2.014986\n",
      "Train Epoch: 8 [16384/22500 (73%)]\tLoss: 2.005666\n",
      "Train Epoch: 8 [20200/22500 (91%)]\tLoss: 1.956124\n",
      "\n",
      "Test set: Average loss: 1.8102, Accuracy: 1133/2500 (45%)\n",
      "\n",
      "\n",
      "Train Epoch: 9 [2048/22500 (9%)]\tLoss: 1.916766\n",
      "Train Epoch: 9 [6144/22500 (27%)]\tLoss: 1.878849\n",
      "Train Epoch: 9 [10240/22500 (45%)]\tLoss: 1.928442\n",
      "Train Epoch: 9 [14336/22500 (64%)]\tLoss: 1.858380\n",
      "Train Epoch: 9 [18432/22500 (82%)]\tLoss: 1.891501\n",
      "\n",
      "Test set: Average loss: 1.7224, Accuracy: 1215/2500 (49%)\n",
      "\n",
      "\n",
      "Train Epoch: 10 [0/22500 (0%)]\tLoss: 1.836333\n",
      "Train Epoch: 10 [4096/22500 (18%)]\tLoss: 1.790177\n",
      "Train Epoch: 10 [8192/22500 (36%)]\tLoss: 1.788715\n",
      "Train Epoch: 10 [12288/22500 (55%)]\tLoss: 1.813005\n",
      "Train Epoch: 10 [16384/22500 (73%)]\tLoss: 1.745839\n",
      "Train Epoch: 10 [20200/22500 (91%)]\tLoss: 1.712001\n",
      "\n",
      "Test set: Average loss: 1.6194, Accuracy: 1274/2500 (51%)\n",
      "\n",
      "\n",
      "Train Epoch: 11 [2048/22500 (9%)]\tLoss: 1.727883\n",
      "Train Epoch: 11 [6144/22500 (27%)]\tLoss: 1.714960\n",
      "Train Epoch: 11 [10240/22500 (45%)]\tLoss: 1.657676\n",
      "Train Epoch: 11 [14336/22500 (64%)]\tLoss: 1.670073\n",
      "Train Epoch: 11 [18432/22500 (82%)]\tLoss: 1.670544\n",
      "\n",
      "Test set: Average loss: 1.5803, Accuracy: 1310/2500 (52%)\n",
      "\n",
      "\n",
      "Train Epoch: 12 [0/22500 (0%)]\tLoss: 1.546603\n",
      "Train Epoch: 12 [4096/22500 (18%)]\tLoss: 1.742396\n",
      "Train Epoch: 12 [8192/22500 (36%)]\tLoss: 1.600706\n",
      "Train Epoch: 12 [12288/22500 (55%)]\tLoss: 1.535580\n",
      "Train Epoch: 12 [16384/22500 (73%)]\tLoss: 1.638741\n",
      "Train Epoch: 12 [20200/22500 (91%)]\tLoss: 1.564116\n",
      "\n",
      "Test set: Average loss: 1.5223, Accuracy: 1360/2500 (54%)\n",
      "\n",
      "\n",
      "Train Epoch: 13 [2048/22500 (9%)]\tLoss: 1.584853\n",
      "Train Epoch: 13 [6144/22500 (27%)]\tLoss: 1.553492\n",
      "Train Epoch: 13 [10240/22500 (45%)]\tLoss: 1.575448\n",
      "Train Epoch: 13 [14336/22500 (64%)]\tLoss: 1.540660\n",
      "Train Epoch: 13 [18432/22500 (82%)]\tLoss: 1.533328\n",
      "\n",
      "Test set: Average loss: 1.4690, Accuracy: 1378/2500 (55%)\n",
      "\n",
      "\n",
      "Train Epoch: 14 [0/22500 (0%)]\tLoss: 1.490573\n",
      "Train Epoch: 14 [4096/22500 (18%)]\tLoss: 1.555194\n",
      "Train Epoch: 14 [8192/22500 (36%)]\tLoss: 1.460295\n",
      "Train Epoch: 14 [12288/22500 (55%)]\tLoss: 1.531563\n",
      "Train Epoch: 14 [16384/22500 (73%)]\tLoss: 1.482551\n",
      "Train Epoch: 14 [20200/22500 (91%)]\tLoss: 1.439157\n",
      "\n",
      "Test set: Average loss: 1.4583, Accuracy: 1389/2500 (56%)\n",
      "\n",
      "\n",
      "Train Epoch: 15 [2048/22500 (9%)]\tLoss: 1.381459\n",
      "Train Epoch: 15 [6144/22500 (27%)]\tLoss: 1.419660\n",
      "Train Epoch: 15 [10240/22500 (45%)]\tLoss: 1.440438\n",
      "Train Epoch: 15 [14336/22500 (64%)]\tLoss: 1.415394\n",
      "Train Epoch: 15 [18432/22500 (82%)]\tLoss: 1.447701\n",
      "\n",
      "Test set: Average loss: 1.4235, Accuracy: 1415/2500 (57%)\n",
      "\n",
      "\n",
      "Train Epoch: 16 [0/22500 (0%)]\tLoss: 1.382929\n",
      "Train Epoch: 16 [4096/22500 (18%)]\tLoss: 1.397600\n",
      "Train Epoch: 16 [8192/22500 (36%)]\tLoss: 1.426860\n",
      "Train Epoch: 16 [12288/22500 (55%)]\tLoss: 1.415581\n",
      "Train Epoch: 16 [16384/22500 (73%)]\tLoss: 1.404034\n",
      "Train Epoch: 16 [20200/22500 (91%)]\tLoss: 1.413163\n",
      "\n",
      "Test set: Average loss: 1.3796, Accuracy: 1453/2500 (58%)\n",
      "\n",
      "\n",
      "Train Epoch: 17 [2048/22500 (9%)]\tLoss: 1.389788\n",
      "Train Epoch: 17 [6144/22500 (27%)]\tLoss: 1.326116\n",
      "Train Epoch: 17 [10240/22500 (45%)]\tLoss: 1.342619\n",
      "Train Epoch: 17 [14336/22500 (64%)]\tLoss: 1.362650\n",
      "Train Epoch: 17 [18432/22500 (82%)]\tLoss: 1.399969\n",
      "\n",
      "Test set: Average loss: 1.3691, Accuracy: 1454/2500 (58%)\n",
      "\n",
      "\n",
      "Train Epoch: 18 [0/22500 (0%)]\tLoss: 1.369883\n",
      "Train Epoch: 18 [4096/22500 (18%)]\tLoss: 1.338795\n",
      "Train Epoch: 18 [8192/22500 (36%)]\tLoss: 1.318107\n",
      "Train Epoch: 18 [12288/22500 (55%)]\tLoss: 1.324013\n",
      "Train Epoch: 18 [16384/22500 (73%)]\tLoss: 1.379586\n",
      "Train Epoch: 18 [20200/22500 (91%)]\tLoss: 1.306677\n",
      "\n",
      "Test set: Average loss: 1.3441, Accuracy: 1487/2500 (59%)\n",
      "\n",
      "\n",
      "Train Epoch: 19 [2048/22500 (9%)]\tLoss: 1.305029\n",
      "Train Epoch: 19 [6144/22500 (27%)]\tLoss: 1.373617\n",
      "Train Epoch: 19 [10240/22500 (45%)]\tLoss: 1.269004\n",
      "Train Epoch: 19 [14336/22500 (64%)]\tLoss: 1.276338\n",
      "Train Epoch: 19 [18432/22500 (82%)]\tLoss: 1.306706\n",
      "\n",
      "Test set: Average loss: 1.3238, Accuracy: 1468/2500 (59%)\n",
      "\n",
      "\n",
      "Train Epoch: 20 [0/22500 (0%)]\tLoss: 1.265525\n",
      "Train Epoch: 20 [4096/22500 (18%)]\tLoss: 1.235251\n"
     ]
    }
   ],
   "source": [
    "train(model, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
